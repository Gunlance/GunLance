## OpenFlow

OpenFlow是一个协议，可以看成是SDN的一个实现

OpenFlow可以定义网络包在交换机中的处理流程（pipeline），因此支持OpenFlow的交换机，其功能不再是固定的，通过OpenFlow可以软件定义OpenVSwitch所具备的功能，

## OVS

虚拟交换机

Open vSwitch（下面简称为 OVS）是由 Nicira Networks 主导的，运行在虚拟化平台（例如 KVM，Xen）上的虚拟交换机
在虚拟化平台上，OVS 可以为动态变化的端点提供 2 层交换功能，很好的控制虚拟网络中的访问策略、网络隔离、流量监控等等

vSwitch 的早期代表是LinuxBridge，它的目的就是为了提供网络连接，模拟了ToR交换机的行为。这样的确是直接套用了现实物理网络的理论和协议，但是带来了虚拟网络与物理网络的耦合性的问题
>TOR（Top of Rack）指的是在每个服务器机柜上部署1～2台交换机，服务器直接接入到本机柜的交换机上，实现服务器与交换机在机柜内的互联。虽然从字面上看，Top of Rack指的是“机柜顶部”，但实际TOR的核心在于将交换机部署在服务器机柜内，既可以部署在机柜顶部，也可以部署在机柜的中部（Middle of Rack）或底部（Bottom of Rack），如图2-1所示。通常而言，将交换机部署在机柜顶部是最有利于走线的，因此这种架构应用最多。

### OpenFlow

OpenVSwitch有一个特点是基于OpenFlow的，由于Openflow的灵活性是建立在可以定义网络包在交换机的处理流程，但是对于复杂功能来说，流程越长，处理一个网络包的需要时间也越长，OVS对此做了很多优化。

对于一个Linux系统来说，可以分为用户空间（user space）和内核空间（kernel space），网络设备接入到内核空间。如果需要将数据传输到用户程序则需要通过内核空间将数据上送到用户空间，如果需要在网络设备之间转发数据，直接在内核空间就可以完成。

作为运行在x86服务器中的软件交换机，直观上来看，应该在内核空间来实现转发。因此，**OpenVSwitch在最早期的时候，在Linux内核模块实现了所有的OpenFlow的处理** 。当时的OpenVSwitch内核模块，接收网络数据包，根据OpenFlow规则，一步步的Match，并根据Action修改网络数据包，最后从某个网络设备送出。

但是这种方式很快证明是不能实际应用的

* 内核进行程序开发与更新更困难，OVS需要大量更新
* 完全按照openflow协议去处理网络包，需要消耗大量的CPU资源，降低网络性能

### OVS(2.X)架构

该版本的的OVS主要由三个部分组成

* ovsdb-server
    * openflow被设计成一个协议，本身不考虑交换机的配置，如Qos，关联SDN控制器等。
    * ovsdb-server是OVS对 **openflow的实现的补充** ，作为OVS的 **configuration database**，保存OVS的持久化数据
* ovs-vswitchd
    * 运行在用户空间的转发程序，接受SDN控制下的Openflow规则，通知ovs内核模块如何处理网络包
* ovs内核模块 
    * 运行在内核空间的转发程序，根据ovs-vswitchd的指示，处理网络数据包

OVS有快速路径（fast path）和慢速路径（slow path），分别对应了ovs内核模块和ovs-vswitchd。openflow存储在slow path中，为了快速转发，网络包需要尽可能咋的fast path中转发，因此：

当一个网络连接的第一个网络数据包（首包）被发出时，OVS内核模块会先收到这个packet。但是内核模块现在还不知道如何处理这个包，因为 **所有的OpenFlow都存在ovs-vswitchd** ，因此它的默认行为是将这个包上送到ovs-vswitchd。
ovs-vswitchd通过OpenFlow pipeline，处理完网络数据包送回给OVS内核模块，同时，ovs-vswitchd还会生成一串类似于OpenFlow Action，但是更简单的 **datapath action** 。这串datapath action会一起送到OVS内核模块。
因为同一个网络连接的所有网络数据包特征（IP，MAC，端口号）都一样，当OVS内核模块收到其他网络包的时候，可以直接应用datapath action。因此，这里将OVS内核模块与OpenFlow协议解耦了，OpenFlow的小改动影响不到内核模块。


### 查找算法

OpenVSwitch，不论是用户空间的ovs-vswitchd，还是内核空间的kernel datapath，最核心都是要实现一个查找算法。

对于ovs-vswitchd，需要根据网络数据包的特征（2-4层包头，metadata）从一个个的OpenFlow Table中查找OpenFlow规则。
对于kernel datapath，也需要根据网络数据包的特征，从cache中查找datapath actions。

OpenVSwitch实现了一个统一的查找算法：TSS（Tuple Space Search），这本质上是一个 **hash 查找算法** 。

TSS优点

* O(1) 的更新时间。前面只看了查找时间，但是SDN控制器同时可能会频繁的更新OpenFlow规则，因此更新时间的性能也很重要。
* hash算法对于任意的OpenFlow Match Field都适用，不论是4层协议的Header还是metadata，还是OpenVSwitch自己扩展的寄存器
* TSS对内存的消耗是线性的，也就是OpenFlow的规则数与TSS消耗的内存成正比

### DPDK

Ovs 有使用 DPDK 作为基础来实现包的转发

<!-- TODO 需要增加页内跳转链接，并完成相关部分 -->

**参考资料**

* https://cloud.tencent.com/developer/article/1198333



## ECMP

ECMP，Equal-Const MultiPath Routing，等价多路由，**是一个逐跳的，基于流的负载均衡策略**


## 名词

<!-- TODO 需要整理 -->

### 网络名词

* Q-router 分布式路由DVR ，qr是 Q-router的网关口，每个子网一个？
    * Qos router？Quality of Service Router？
    * 对流量进行服务优先级配置，达到限速和保证高优先级别流量通过拥塞路径上的路由设备
* V-Router 网关口是 sg
* qvm port？
* TOP
* TOR   top of rack 交换机放在机架的最上面
* SPI
* CPS Cloud Provisioning Service

### 其他

* FusionCloud   桌面云解决方案
    * FC    Fusion Compute
    * FM    Fusion Manager 
    * FA    Fusion Access 桌面管理软件
    * F？   Fusion Cube 融合解决方案
* VNC 在虚拟机网络不通的情况下，可以通过VNC登录，先登录到虚拟机所在的宿主机，再通过内部通道访问至主机


<font color=d55fde>*我很好奇这么多口是用来干嘛的。。。*</font>

